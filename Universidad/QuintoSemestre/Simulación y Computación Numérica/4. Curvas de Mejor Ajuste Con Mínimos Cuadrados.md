<<<<<<< HEAD
Sean los datos $(x_1, y_1),(x_2,y_2),...,(x_m,y_m)$, hallar la *recta* (polinomio de grado 1) que mejor se ajusta a los datos. Para $P_1(x) = a_0 + a_1\cdot x$, hallar $a_0$ y $a_1$ tales que:
=======
Sean los datos $(x_1, y_1),(x_2,y_2),...,(x_m,y_m)$, hallar la *recta* (polinomio de grado 1) que mejor se ajusta a los datos. Para $P_1(x) = a_0 + a_1\cdot x$ hallar $a_0$ y $a_1$ tales que:
>>>>>>> 5434d384d4d954190420ee09affe295992558656
$$\sum_{i=1}^{m}(P_1(x_i)-y_i)^2$$
sea mínima. Para la resolución, puesto que es conocido que $P_1(x) = a_0 + a_1\cdot x$, podemos plantear la sumatoria anterior de la siguiente manera:
$$g(a_0,a_1)=\sum_{i=1}^{m}(a_0+a_1x_i-y_i)^2$$
Nótese que, a diferencia de $P_1$, la expresión $g$ está en función de $a_0$ y $a_1$. Así, podemos obtener las siguientes derivadas parciales:

1. Derivada parcial respecto a $a_0$:
$$\frac{\partial g}{\partial a_0} = \frac{\partial}{\partial a_0} \left[\sum_{i=1}^{m}(a_0+a_1x_i-y_i)^2\right] = \sum_{i=1}^{m}\frac{\partial}{\partial a_0}(a_0+a_1x_i-y_i)^2 $$
$$
= \sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i)
$$
Igualamos a cero para obtener nuestra primera ecuación:
$$
\sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i) = 0 \tag 1
$$
2. Derivada parcial respecto a $a_0$:
$$\frac{\partial g}{\partial a_1} = \frac{\partial}{\partial a_1} \left[\sum_{i=1}^{m}(a_0+a_1x_i-y_i)^2\right] = \sum_{i=1}^{m}\frac{\partial}{\partial a_1}(a_0+a_1x_i-y_i)^2 $$
$$
= \sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i)\cdot x_i
$$
Igualamos a cero para obtener la segunda ecuación:
$$
\sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i)\cdot x_i = 0 \tag 2
$$

Tras operar sobre las ecuaciones y simplificarlas, obtenemos expresiones conocidas como **Ecuaciones normales** o **Ecuaciones de Gauss**.
1. Para la expresión $(1)$:
$$
\begin{align}
	&\sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i) = 0 \tag 1 \\[8pt]
	
	=&2\cdot\sum_{i=1}^{m}(a_0+a_1x_i-y_i) = 0 \\[8pt]
	
	=&2\cdot \left[\sum_{i=1}^{m}a_0+\sum_{i=1}^{m}a_1x_i-\sum_{i=1}^{m}y_i\right] = 0 \\[8pt]
	
	=&2\cdot \left[\left(\sum_{i=1}^{m}1\right)a_0+\left(\sum_{i=1}^{m}x_i\right)a_1-\left(\sum_{i=1}^{m}1\right)y_i\right] = 0 \\[8pt]
	
	=&\frac{2}{2}\cdot \left[\left(\sum_{i=1}^{m}1\right)a_0+\left(\sum_{i=1}^{m}x_i\right)a_1-\left(\sum_{i=1}^{m}1\right)y_i\right] = \frac{0}{2} \quad \text{Se divide la expresión entre 2.} \\[8pt]
	
	=&\left(\sum_{i=1}^{m}1\right)a_0+\left(\sum_{i=1}^{m}x_i\right)a_1-\left(\sum_{i=1}^{m}1\right)y_i = 0 \\[8pt]
	
	=&\left(\sum_{i=1}^{m}1\right)a_0+\left(\sum_{i=1}^{m}x_i\right)a_1 =\sum_{i=1}^{m}y_i \tag 1 \\
\end{align}
$$
2. Para la expresión $(2)$:
$$
\begin{align}
	&\sum_{i=1}^{m}2\cdot(a_0+a_1x_i-y_i) \cdot x_i = 0 \tag 2 \\[8pt]
	
	=&2\cdot\sum_{i=1}^{m}(a_0x_i+a_1x_i^2-y_ix_i) = 0 \\[8pt]
	
	=&2\cdot \left[\sum_{i=1}^{m}a_0x_i+\sum_{i=1}^{m}a_1x_i^2-\sum_{i=1}^{m}y_ix_i\right] = 0 \\[8pt]	
	
	=&2\cdot \left[\left(\sum_{i=1}^{m}x_i\right)a_0+\left(\sum_{i=1}^{m}x_i^2\right)a_1-\left(\sum_{i=1}^{m}x_i\right)y_i\right] = 0 \\[8pt]
	
	=&\frac{2}{2}\cdot \left[\left(\sum_{i=1}^{m}x_i\right)a_0+\left(\sum_{i=1}^{m}x_i^2\right)a_1-\left(\sum_{i=1}^{m}x_i\right)y_i\right] = \frac{0}{2} \quad \text{Se divide la expresión entre 2.} \\[8pt]
	
	=&\left(\sum_{i=1}^{m}x_i\right)a_0+\left(\sum_{i=1}^{m}x_i^2\right)a_1-\left(\sum_{i=1}^{m}x_i\right)y_i = 0 \\[8pt]
	
	=&\left(\sum_{i=1}^{m}x_i\right)a_0+\left(\sum_{i=1}^{m}x_i^2\right)a_1 =\sum_{i=1}^{m}y_ix_i \tag 2 \\
\end{align}
$$
3. Se tienen las Ecuaciones naturales o Ecuaciones de Gauss:
$$
\text{Ecuaciones de Gauss}
\begin{cases}
	
	\left(\sum_{i=1}^{m}1\right)a_0+\left(\sum_{i=1}^{m}x_i\right)a_1 =\sum_{i=1}^{m}y_i \\[8pt]
	
	\left(\sum_{i=1}^{m}x_i\right)a_0+\left(\sum_{i=1}^{m}x_i^2\right)a_1 =\sum_{i=1}^{m}y_ix_i
\end{cases}
$$
4. Vista matricial simplificada[^1] de las Ecuaciones de Gauss:
$$
\begin{bmatrix}
\sum 1 & \sum x_i \\[8pt]
\sum x_i & \sum x_i^2
\end{bmatrix}
%
\begin{bmatrix}
a_0 \\[9.5pt]
a_1
\end{bmatrix}
%
=
%
\begin{bmatrix}
\sum y_1 \\[9.5pt]
\sum y_ix_i
\end{bmatrix}
$$
[^1]: La notación de las sumatorias está simplificada. Debe sobreentenderse que, en estas vistas matriciales, la notación $\sum$ es equivalente a $\sum_{i=1}^m$.
